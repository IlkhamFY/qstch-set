%=============================================================================
% THEORY SECTION â€” Trimmed for 8-page NeurIPS submission
% Full proofs are in appendix.tex
%=============================================================================

\section{Theoretical Analysis}
\label{sec:theory}

We establish the theoretical properties of qSTCH-Set as a BO acquisition function.
We present rigorous results (Propositions~\ref{prop:valid}--\ref{prop:pareto-transfer})
and discuss empirical consistency (Remark~\ref{conj:consistency}), leaving the formal $K > 1$ proof as future work.
Throughout, $\X \subset \R^d$ is compact and $f_1,\ldots,f_m:\X\to\R$ are the true (unknown) objectives.

%--------------------------------------------------------------------
\subsection{Definitions}
\label{sec:theory-defs}
%--------------------------------------------------------------------

\begin{definition}[Smooth Tchebycheff Set scalarization]
\label{def:stchset}
For candidate set $\Xset = \{\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(K)}\}\subset\X$,
preference vector $\boldsymbol{\lambda}\in\Delta^{m-1}_{++}$,
reference point $\mathbf{z}^*\in\R^m$, and smoothing parameter $\mu>0$:
\begin{align}
g_\mu^{\mathrm{STCH\text{-}Set}}(\Xset\mid\boldsymbol{\lambda})
&= \mu\log\!\left(\sum_{i=1}^{m}\exp\!\left(
    \frac{\lambda_i\bigl(\smin_\mu^{(k)} f_i(\mathbf{x}^{(k)}) - z_i^*\bigr)}{\mu}
\right)\right), \label{eq:stchset-def}\\
\text{where}\quad
\smin_\mu^{(k)} f_i(\mathbf{x}^{(k)})
&= -\mu\log\!\left(\sum_{k=1}^{K}\exp\!\left(-\frac{f_i(\mathbf{x}^{(k)})}{\mu}\right)\right).
\label{eq:smin-def}
\end{align}
\end{definition}

\begin{definition}[qSTCH-Set acquisition function]
\label{def:qstchset}
Given GP posteriors $\hat{f}_1^{(t)},\ldots,\hat{f}_m^{(t)}$ after $t$ observations,
the \emph{qSTCH-Set acquisition function} is:
\begin{equation}
\label{eq:acq}
\alpha_t^{\mathrm{qSTCH}}(\Xset)
= \E_{\boldsymbol{\omega}}\!\left[
    -g_\mu^{\mathrm{STCH\text{-}Set}}\!\left(
        \hat{\mathbf{f}}_\omega^{(t)}(\Xset)\mid\boldsymbol{\lambda}
    \right)
\right],
\end{equation}
approximated in practice by $N$ quasi-Monte Carlo base samples.
\end{definition}

%--------------------------------------------------------------------
\subsection{Proposition 1: qSTCH-Set Is a Valid MC Acquisition Function}
\label{sec:theory-prop1}
%--------------------------------------------------------------------

\begin{proposition}[Validity as MC acquisition function]
\label{prop:valid}
The qSTCH-Set acquisition function $\alpha_t^{\mathrm{qSTCH}}(\Xset)$
defined in~\eqref{eq:acq} satisfies: (a)~\textbf{Measurability}---the integrand is Borel-measurable
in the base samples $\boldsymbol{\omega}$; (b)~\textbf{Finite expectation}---$\E[|\alpha_t^{\mathrm{qSTCH}}(\Xset)|]<\infty$
for GPs with continuous kernels on compact domains; (c)~\textbf{Differentiability}---$\alpha_t^{\mathrm{qSTCH}}(\Xset)$
is differentiable in $\Xset$ via the reparameterization trick, with gradient
\begin{equation}
\label{eq:grad}
\nabla_{\mathbf{x}^{(k)}}\, g_\mu^{\mathrm{STCH\text{-}Set}}
= \sum_{i=1}^m w_i \cdot p_{ik}\cdot \nabla f_i(\mathbf{x}^{(k)}),
\end{equation}
where $w_i$ and $p_{ik}$ are the softmax attention weights over objectives and candidates, respectively.
\end{proposition}

\begin{proof}[Proof sketch]
(a)~The posterior sample paths are affine in the base samples via the reparameterization trick; STCH-Set is a composition of $\exp$, $\log$, and finite sums, hence measurable.
(b)~GPs on compact sets have a.s.\ bounded sample paths; $|g_\mu^{\mathrm{STCH\text{-}Set}}| \le M + |z_{\max}^*| + \mu\log m$ for finite $M$.
(c)~The scalarization is $C^\infty$ with strictly positive log arguments; the gradient~\eqref{eq:grad} follows by the chain rule on the nested log-sum-exp structure, and differentiation under the integral sign is justified by dominated convergence.
Full proof in Appendix~\ref{app:proof-valid}.
\end{proof}

\begin{remark}[Comparison to non-smooth Chebyshev]
The classical Tchebycheff scalarization uses a hard $\max$, which is non-differentiable when objectives tie,
preventing direct reparameterization-trick gradient estimation.
STCH-Set is $C^\infty$ for all $\mu>0$, enabling standard L-BFGS-B acquisition optimization,
unlike qNParEGO~\cite{daulton2020qnehvi} which requires heuristic smoothing.
\end{remark}

%--------------------------------------------------------------------
\subsection{Proposition 2: Pareto Optimality of Acquisition Maximizers}
\label{sec:theory-prop2}
%--------------------------------------------------------------------

\begin{assumption}[GP surrogate regularity]
\label{asm:gp}
Each $f_i$ is modeled by an independent GP with a continuous positive-definite kernel $k_i$ on $\X\subset\R^d$.
\end{assumption}

\begin{proposition}[Pareto optimality under the posterior]
\label{prop:pareto-transfer}
Let Assumption~\ref{asm:gp} hold, $\boldsymbol{\lambda}\in\Delta^{m-1}_{++}$, and define
$\hat{g}_\mu^{(t)}(\Xset\mid\boldsymbol{\lambda}) := g_\mu^{\mathrm{STCH\text{-}Set}}(\Xset\mid\boldsymbol{\lambda})\big|_{f_i=\mu_i^{(t)}}$.
Then:
\begin{enumerate}
    \item[\textnormal{(a)}]
    \textbf{Surrogate Pareto optimality.}
    If $\Xset^*=\arg\min_{\Xset\in\X^K}\hat{g}_\mu^{(t)}(\Xset\mid\boldsymbol{\lambda})$,
    every $\mathbf{x}^{(k)}\in\Xset^*$ is weakly Pareto optimal w.r.t.\ $(\mu_1^{(t)},\ldots,\mu_m^{(t)})$
    (Pareto optimal if $\Xset^*$ is unique).

    \item[\textnormal{(b)}]
    \textbf{Approximation bound.}
    \begin{equation}
    \label{eq:sandwich}
    g^{\mathrm{TCH\text{-}Set}}(\Xset\mid\boldsymbol{\lambda})
    \;\le\;
    g_\mu^{\mathrm{STCH\text{-}Set}}(\Xset\mid\boldsymbol{\lambda})
    \;\le\;
    g^{\mathrm{TCH\text{-}Set}}(\Xset\mid\boldsymbol{\lambda}) + \mu\log m + \mu\log K.
    \end{equation}
\end{enumerate}
\end{proposition}

\begin{proof}[Proof sketch]
(a)~The posterior means $\mu_i^{(t)}$ satisfy the differentiability requirements of \cite[Theorem~2]{lin2025few}; since $\lambda_i>0$, all hypotheses hold and the Pareto optimality conclusion transfers directly to the surrogate setting.
(b)~The lower bound follows since log-sum-exp upper-bounds max. For the upper bound: the outer log-sum-exp contributes $+\mu\log m$ (standard bound~\cite{lin2024smooth}), and the inner smooth minimum contributes $+\mu\log K$ per objective (from $\sum_k e^{-b_k/\mu}\le K e^{-\min_k b_k/\mu}$); combining gives the $\mu\log(mK)$ gap.
Full proof in Appendix~\ref{app:proof-pareto}.
\end{proof}

\begin{corollary}[Asymptotic quality with $\mu$-annealing]
\label{cor:annealing}
If $\mu_t = c/\log(t+1)$, then $\mu_t\log(mK)\to 0$ as $t\to\infty$,
so solutions converge in scalarization value to the true TCH-Set optimum.
\end{corollary}

%--------------------------------------------------------------------
\subsection{Empirical Consistency Under GP Posterior Convergence}
\label{sec:theory-conj}
%--------------------------------------------------------------------

\begin{remark}[Empirical consistency of qSTCH-Set BO]
\label{conj:consistency}
Empirically, qSTCH-Set solutions converge toward the Pareto front as the number of BO iterations increases (Figures~\ref{fig:convergence_m5}--\ref{fig:convergence_m8} and Appendix~\ref{app:extended}).
Three ingredients support a formal consistency argument---that every $\mathbf{x}^{(k)}\in\Xset_t^*$ becomes $\varepsilon_t$-Pareto optimal with $\varepsilon_t \to 0$:
\begin{enumerate}
    \item \textbf{GP posterior concentration}~\cite{srinivas2010ucb}: with high probability, $|f_i(\mathbf{x})-\mu_i^{(t)}(\mathbf{x})|\le\beta_t^{1/2}\sigma_i^{(t)}(\mathbf{x})\to 0$ as the model concentrates.
    \item \textbf{Lipschitz stability} of STCH-Set in the objective values transfers posterior errors to scalarization errors at rate $O(\beta_t^{1/2}\bar{\sigma}_t)$.
    \item For $K{=}1$, consistency is proved by the composite BO result of \cite{astudillo2019composite}. For $K{>}1$, the set-valued optimization introduces permutation symmetries requiring set-valued epi-convergence arguments (Rockafellar \& Wets, Ch.~7) that remain open.
\end{enumerate}
We leave the formal proof for $K > 1$ as future work.
\end{remark}

%--------------------------------------------------------------------
\subsection{Computational Complexity}
\label{sec:theory-complexity}
%--------------------------------------------------------------------

\begin{proposition}[Per-evaluation complexity]
\label{prop:complexity}
qSTCH-Set with $K$ candidates, $m$ objectives, and $N$ MC samples runs in $O(NKm)$ time,
compared to $O(N 2^m)$ or worse for hypervolume-based methods and $O(Nm)$ for qNParEGO
(which, however, lacks set-based coordination).
\end{proposition}

\begin{proof}
The STCH-Set scalarization requires: (1)~$O(Km)$ for weighted deviations; (2)~$O(Km)$ for smooth min via logsumexp over $K$ per objective; (3)~$O(m)$ for smooth max via logsumexp over objectives. Summing over $N$ samples gives $O(NKm)$; gradients follow by automatic differentiation at the same cost.
\end{proof}
